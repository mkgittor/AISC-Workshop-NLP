# -*- coding: utf-8 -*-
"""assignment_2 - Tranformers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a0emL2aBxH1OANEcoF1EnA8hdXaAKzC7
"""

# This notebook is part of the workshop "modern natural language processing" run 
# by Aggregate Intellect Inc. (https://ai.science), and is released under 
# 'Creative Commons Attribution-NonCommercial-ShareAlike CC BY-NC-SA" license. 
# This material can be altered and distributed for non-commercial use with 
# reference to Aggregate Intellect Inc. as the original owner, and any material 
# generated from it must be released under similar terms. 
# (https://creativecommons.org/licenses/by-nc-sa/4.0/)

"""# download data from host"""

!wget "https://ai-modern-nlp-workshop.s3.amazonaws.com/datasets/glove.6B.zip" -O "glove.6B.50d.zip"
!unzip /content/glove.6B.50d.zip
!wget "https://ai-modern-nlp-workshop.s3.amazonaws.com/datasets/train.csv.zip" -O "train.zip"
!unzip /content/train.zip
!wget "https://drive.google.com/uc?id=15S3OXTbRdAlYIx2_R2RnI7l2CzszQOdu" -O "train.csv"

#!wget "https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/Batch.py" -O "Batch.py"
#!wget "https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/Tokenize.py" -O 'Tokenize.py'
#!wget "https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt" -O "english.txt"
#!wget "https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt" -O 'french.txt'

#europarl_en = open('english.txt', encoding='utf-8').read().strip().split('\n')
#europarl_fr = open('french.txt', encoding='utf-8').read().strip().split('\n')

"""we import several libraries:
- keras is used for text preprocessing
- torch is for building the LSTM neural net
- gensim is for loading pre-trained Glove word embeddings
"""

import re
import csv
import codecs
import numpy as np
import pandas as pd

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from torch.utils.data import DataLoader, TensorDataset
import torch
import torch.nn as nn
import torch.nn.functional as F
import spacy

from gensim.models import KeyedVectors


import torch
import torch.nn as nn
from torch.autograd import Variable

import spacy
import torchtext
from torchtext import data
from torchtext.data import Field, BucketIterator, TabularDataset

from sklearn.model_selection import train_test_split

# from Batch import MyIterator, batch_size_fn
# from Tokenize import tokenize

import os

#import numpy as np
#import pandas as pd

import math
import copy

#import torch.nn.functional as F
import time
#!python -m spacy download fr

"""# reading in the data
we hard code a dictionary to clean punctuated words and do the text cleaning as we read in the training file
"""

def text_to_wordlist(text):    
    """Convert words to lower case, split them, and clean punctuated words using a hard coded dictionary."""
    text = text.lower().split()
    text = " ".join(text)

    # Clean the text
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)
    
    return(text)

# read in the training data and clean the text
texts_1 = [] 
texts_2 = []
labels = []
with codecs.open('train.csv', encoding='utf-8') as f:
    reader = csv.reader(f, delimiter=',')
    header = next(reader)
    for values in reader:
        texts_1.append(text_to_wordlist(values[3]))
        texts_2.append(text_to_wordlist(values[4]))
        labels.append(int(values[5]))
print(f'Found {len(texts_1)} texts in train.csv')
print('We visualize the actual data: \n ')
print('\n', texts_1[0])
print('\n', texts_2[0])
print('\n', labels[0])

"""# tokenizing, word indexing and padding
In the following block, we do the following procedures:

1) tokenize the sentences: convert a sentence to list of words

2) word indexing: convert word(s) to integer(s)

3) padding and truncating the sentences to the same length of 30 words
"""

# tokenizing and indexing in the same step: converting sentence into words, then into integers
tokenizer = Tokenizer(num_words=200000)
tokenizer.fit_on_texts(texts_1 + texts_2) # training
sequences_1 = tokenizer.texts_to_sequences(texts_1) # predicting
sequences_2 = tokenizer.texts_to_sequences(texts_2) # predicting
print('\n')
print(f"sequences_1: {sequences_1[0]}")

word_index = tokenizer.word_index
print('\n')
print(f'Found {len(word_index)} unique tokens')
# print(word_index) # uncomment to see the word dictionary

# padding/truncating sentences to the same length of 30 words
max_sequence_length = 30
data_1 = torch.tensor(pad_sequences(sequences_1, maxlen=max_sequence_length)).cuda() # Question_1, train
data_2 = torch.tensor(pad_sequences(sequences_2, maxlen=max_sequence_length)).cuda() # Question_2, train
print('\n')
print(f"data_1: {data_1[0]}")

"""# embeddings

Because pre-trained glove embeddings is not in a format to be loaded into pytorch, we need to convert the format first
"""

!python -m gensim.scripts.glove2word2vec --input glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt

"""We load the pre-trained glove embeddings into Pytorch then query the embeddings"""

model = KeyedVectors.load_word2vec_format('glove.6B.50d.w2vformat.txt')
weights = torch.FloatTensor(model.vectors)
embedding = nn.Embedding.from_pretrained(weights)
print(embedding, '\n')

# query the embeddings
# TODO: finish the below so that we can query the pre-trained embeddings. Utilize the word_index dictionary we created.
input = torch.LongTensor([233])
# print(word_index)
# other example words: 'them 136', 'company 177', 'during 233'
print(f"query the embedding: \n{embedding(input)}")

"""# creating batches of training/validation/testing data

Here we unify question 1 and question 2, length of 30 respectively, into 1 large sentence, length of 60
We will later feed this unified sentence into LSTM as input
"""

# NOTE: if we feed question 1 and question 2 separately into 2 diff network, performance would improve
# But we take a simpler approach here by unifying the 2 questions into 1 sentence, and feed it into 1 network
data = torch.cat((data_1, data_2), 1)
labels = torch.tensor(labels)
print(data_1.size(), data_2.size())
print(data.size())
print(labels.size())

"""Then, we split the data into 80-10-10 (train-val_test).

We also use the pytorch TensorDataset method to create a dataset format readable by PyTorch models,

followed by uisng DataLoader function to create batches of data.

We do these procedures for all training, validation, and testing set.
"""

# split data into trainining, validation, testing set
split_frac = 0.8
len_feat = len(data)
train_x = data[0:int(split_frac*len_feat)]
train_y = labels[0:int(split_frac*len_feat)]
temp_remain_x = data[int(split_frac*len_feat):]
temp_remain_y = labels[int(split_frac*len_feat):]
valid_x = temp_remain_x[0:int(len(temp_remain_x)*0.5)]
valid_y = temp_remain_y[0:int(len(temp_remain_y)*0.5)]
test_x = temp_remain_x[int(len(temp_remain_x)*0.5):]
test_y = temp_remain_y[int(len(temp_remain_y)*0.5):]
print(len(train_x), len(valid_x), len(test_x))
assert len(train_x) + len(valid_x) + len(test_x) == len(data), 'Two lengths not equal'

# set up torch dataset
train_data = TensorDataset(train_x, train_y)
# TODO: complete the tensor setup for validation data
valid_data =  TensorDataset(valid_x, valid_y)
test_data = TensorDataset(test_x, test_y)
print(train_data, valid_data, test_data)

# batching
batch_size = 50
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)
# TODO: complete the batching for validation data
valid_loader =  DataLoader(valid_data, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)

# sanity check to see if we have consisntet label of 0 and 1 for our train/val/test set
print('labels:\n', pd.Series(labels.numpy()).value_counts() / len(labels), '\n')
print('train:\n', pd.Series(train_y.numpy()).value_counts() / len(train_y), '\n')
print('val:\n', pd.Series(valid_y.numpy()).value_counts() / len(valid_y), '\n')
print('test:\n', pd.Series(test_y.numpy()).value_counts() / len(test_y), '\n')

"""Here we do 1 more sanity check to see if we can get one batch of training data"""

iter_data = iter(train_loader)
temp_x, temp_y = iter_data.next()
print('One batch of input: \n', temp_x)
print('One batch of input size: ', temp_x.size())
print('\n')
print('One batch of label: \n', temp_y)
print('One batch of label size: ', temp_y.size())
print(len(word_index))

"""# Transformer

We build the actual LSTM.

Embedding layer takes the input. The embedding layer can be chosen as using a pre-trained embedding or trained along with the network.

Two hidden layers.

One drop out layer.

One fully connected layer.

Final sigmoid to produce the output.
"""

class Embedder(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)
      
class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_seq_len = 80):
        super().__init__()
        self.d_model = d_model
        
        # create constant 'pe' matrix with values dependant on 
        # pos and i
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i)/d_model)))
                pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
                
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
 
    
    def forward(self, x):
        # make embeddings relatively larger
        x = x * math.sqrt(self.d_model)
        #add constant to embedding
        seq_len = x.size(1)
        x = x + Variable(self.pe[:,:seq_len], \
        requires_grad=False).cuda()
        return x
      
#Input masking

#batch = next(iter(train_loader))
#input_seq = batch.English.transpose(0,1)
#input_pad = EN_TEXT.vocab.stoi['<pad>']
# creates mask with 0s wherever there is padding in the input
#input_msk = (input_seq != input_pad).unsqueeze(1)

# Target masking

# create mask as before
#target_seq = batch.French.transpose(0,1)
#target_pad = FR_TEXT.vocab.stoi['<pad>']
#target_msk = (target_seq != target_pad).unsqueeze(1)
#size = target_seq.size(1) # get seq_len for matrix
#nopeak_mask = np.triu(np.ones(1, size, size),
#k=1).astype('uint8')
#nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)
#target_msk = target_msk & nopeak_mask

# Adding Multihead attention , code for decoder layer 

class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.d_k = d_model // heads
        self.h = heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)
    
    def forward(self, q, k, v, mask=None):
        
        bs = q.size(0)
        
        # perform linear operation and split into h heads
        
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)
        
        # transpose to get dimensions bs * h * sl * d_model
       
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)
# calculate attention using function we will define next
        scores = attention(q, k, v, self.d_k, mask, self.dropout)
        
        # concatenate heads and put through final linear layer
        concat = scores.transpose(1,2).contiguous()\
        .view(bs, -1, self.d_model)
        
        output = self.out(concat)
    
        return output
    
#Attention function in encoder

def attention(q, k, v, d_k, mask=None, dropout=None):
    
    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)
    if mask is not None:
        mask = mask.unsqueeze(1)
        scores = scores.masked_fill(mask == 0, -1e9)
    scores = F.softmax(scores, dim=-1)
    
    if dropout is not None:
        scores = dropout(scores)
        
    output = torch.matmul(scores, v)
    return output

#Final layer with Relu and dropout

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048, dropout = 0.1):
        super().__init__() 
        # We set d_ff as a default to 2048
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)
    def forward(self, x):
        x = self.dropout(F.relu(self.linear_1(x)))
        x = self.linear_2(x)
        return x
      
# Noralizing for smoother results

class Norm(nn.Module):
    def __init__(self, d_model, eps = 1e-6):
        super().__init__()
    
        self.size = d_model
        # create two learnable parameters to calibrate normalisation
        self.alpha = nn.Parameter(torch.ones(self.size))
        self.bias = nn.Parameter(torch.zeros(self.size))
        self.eps = eps
    def forward(self, x):
        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
        return norm
      
# Finally building the encoder and decoder layer 

# build an encoder layer with one multi-head attention layer and one # feed-forward layer
class EncoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout = 0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.attn = MultiHeadAttention(heads, d_model)
        self.ff = FeedForward(d_model)
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        
    def forward(self, x, mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.ff(x2))
        return x
    
# build a decoder layer with two multi-head attention layers and
# one feed-forward layer
class DecoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.norm_3 = Norm(d_model)
        
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        self.dropout_3 = nn.Dropout(dropout)
        
        self.attn_1 = MultiHeadAttention(heads, d_model)
        self.attn_2 = MultiHeadAttention(heads, d_model)
        self.ff = FeedForward(d_model).cuda()
    def forward(self, x, e_outputs, src_mask, trg_mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,
        src_mask))
        x2 = self.norm_3(x)
        x = x + self.dropout_3(self.ff(x2))
        return x
# We can then build a convenient cloning function that can generate multiple layers:
def get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])
  

# Building the encoder and decoder 

class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(EncoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
    def forward(self, src, mask=None):
        x = self.embed(src)
        x = self.pe(x)
        for i in range(N):
            x = self.layers[i](x, mask)
        return self.norm(x)
    
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(DecoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
    def forward(self, trg, e_outputs, src_mask, trg_mask):
        x = self.embed(trg)
        x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, e_outputs, src_mask, trg_mask)
        return self.norm(x)


# Transformer built here      
      
class Transformer(nn.Module):
    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, heads)
        #self.decoder = Decoder(trg_vocab, d_model, N, heads)
        self.out = nn.Linear(d_model,trg_vocab)
        self.sig = nn.Sigmoid()
    def forward(self, src):
        e_outputs = self.encoder(src)
        #d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)
        troutput = self.out(e_outputs)
        
        # sigmoid function
        sig_out = self.sig(troutput)
        
        # reshape to be batch_size first
        sig_out = sig_out.view(batch_size, -1)
        # convert 2D to 1D
        sig_out = sig_out[:, -1]
        
        # return last sigmoid output
        return sig_out

      
# we don't perform softmax on the output as this will be handled 
# automatically by our loss function

"""Initialize the network here. Depending on if you use pre-trained embeddings or not, the network will be slightly different as shown below:

Transformer(
  (encoder): Encoder(
    (embed): Embedder(
      (embed): Embedding(85519, 256)
    )
    (pe): PositionalEncoder()
    (layers): ModuleList(
      (0): EncoderLayer(
        (norm_1): Norm()
        (norm_2): Norm()
        (attn): MultiHeadAttention(
          (q_linear): Linear(in_features=256, out_features=256, bias=True)
          (v_linear): Linear(in_features=256, out_features=256, bias=True)
          (k_linear): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1)
          (out): Linear(in_features=256, out_features=256, bias=True)
        )
        (ff): FeedForward(
          (linear_1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1)
          (linear_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (dropout_1): Dropout(p=0.1)
        (dropout_2): Dropout(p=0.1)
      )
      (1): EncoderLayer(
        (norm_1): Norm()
        (norm_2): Norm()
        (attn): MultiHeadAttention(
          (q_linear): Linear(in_features=256, out_features=256, bias=True)
          (v_linear): Linear(in_features=256, out_features=256, bias=True)
          (k_linear): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.1)
          (out): Linear(in_features=256, out_features=256, bias=True)
        )
        (ff): FeedForward(
          (linear_1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1)
          (linear_2): Linear(in_features=2048, out_features=256, bias=True)
        )
        (dropout_1): Dropout(p=0.1)
        (dropout_2): Dropout(p=0.1)
      )
    )
    (norm): Norm()
  )
  (out): Linear(in_features=256, out_features=1, bias=True)
  (sig): Sigmoid()
)

"""

   
# Transformer piece

d_model = 256
heads = 4
N = 2
src_vocab = len(word_index) + 1
trg_vocab = 1
#trg_vocab = len(FR_TEXT.vocab)
model = Transformer(src_vocab, trg_vocab, d_model, N, heads)
print(model)
for p in model.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)
# this code is very important! It initialises the parameters with a
# range of values that stops the signal fading or getting too big.
# See this blog for a mathematical explanation.
optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

"""# training

Below is the training procedure.

We use binary cross entropy loss and adam optimizer.

Pytorch has the training pattern:
  - set the model to trainable
  - forward propagation
  - backpropagation
  - calculate the loss
  - run the optimizer
  
We also run a validation every epoch
"""

# training parameters
#lr = 0.002
# TODO: specify binary cross entropy loss
criterion = nn.BCELoss() # NOTE: assignment section (6)
#optimizer = torch.optim.Adam(model.parameters(), lr=lr)

epochs = 5
counter = 0
clip = 5
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

# train for some number of epochs
for e in range(epochs):
    model.train()
    # initialize hidden state
    #h = net.init_hidden(batch_size)

    # batch loop
    for inputs, labels in train_loader:
        if len(inputs) != batch_size:
          continue
        counter += 1

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        #h = tuple([each.data for each in h])

        # zero accumulated gradients
        model.zero_grad()

        # get the output from the model
        inputs = inputs.type(torch.LongTensor)
        inputs = inputs.to(device)
        labels = labels.to(device)
        output = model(inputs)
        #print(output)
        # calculate the loss and perform backprop
        # TODO: calculate the loss between predicted and labels
        loss = criterion(output.squeeze(), labels.float()) # NOTE: assignment section (7)
        loss.backward()
        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
        nn.utils.clip_grad_norm_(model.parameters(), clip)
        optim.step()

    # Get validation loss
    #val_h = net.init_hidden(batch_size)
    val_losses = []
    val_acc = []
    model.eval()
    for inputs, labels in valid_loader:
        if len(inputs) != batch_size:
          continue

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        #val_h = tuple([each.data for each in val_h])

        inputs = inputs.type(torch.LongTensor)
        inputs, labels = inputs.cuda(), labels.cuda()
        output = model(inputs)
        val_loss = criterion(output.squeeze(), labels.float())
        
        # accuracy
        output = (output > 0.5).float()
        output = output.type(torch.LongTensor)
        correct = (output.cpu() == labels.cpu()).float().sum()

        val_losses.append(val_loss.item())
        val_acc.append(correct / output.shape[0])

        model.train()
        
    print("Epoch: {}/{}...".format(e+1, epochs),
          "Step: {}...".format(counter),
          "Loss: {:.6f}...".format(loss.item()),
          "Val Loss: {:.6f}".format(np.mean(val_losses)),
          "Val Accuracy: {:.6f}".format(np.mean(val_acc)))

"""
Model dimension - 256 and 5 Epochs

Epoch: 1/5... Step: 6468... Loss: 0.471713... Val Loss: 0.460801 Val Accuracy: 0.780173
Epoch: 2/5... Step: 12936... Loss: 0.474539... Val Loss: 0.434694 Val Accuracy: 0.796163
Epoch: 3/5... Step: 19404... Loss: 0.348721... Val Loss: 0.428224 Val Accuracy: 0.800743
Epoch: 4/5... Step: 25872... Loss: 0.268656... Val Loss: 0.430793 Val Accuracy: 0.807327
Epoch: 5/5... Step: 32340... Loss: 0.409043... Val Loss: 0.443851 Val Accuracy: 0.799035

"""

"""# test Set

Finally we run the model through a hold out test set.

The procedure is the same as the validation process.
"""

# check accuracy
#test_h = net.init_hidden(batch_size)
test_losses = []
test_acc = []
model.eval()
for inputs, labels in test_loader:
    if len(inputs) != batch_size:
      continue

    # Creating new variables for the hidden state, otherwise
    # we'd backprop through the entire training history
    #test_h = tuple([each.data for each in test_h])

    inputs = inputs.type(torch.LongTensor)
    inputs, labels = inputs.cuda(), labels.cuda()
    output = model(inputs)
    test_loss = criterion(output.squeeze(), labels.float())
    output = (output > 0.5).float()
    output = output.type(torch.LongTensor)
    correct = (output.cpu() == labels.cpu()).float().sum()

    test_losses.append(test_loss.item())
    test_acc.append(correct / output.shape[0])

model.train()
print("Test Loss: {:.6f}".format(np.mean(test_losses)),
      "\n"
      "Test Accuracy: {:.6f}".format(np.mean(test_acc))
      )

"""
Model dimension - 256 and 5 Epochs

Test Loss: 0.438555 
Test Accuracy: 0.803490

"""

"""# challenges
- how can we improve the performance?
  - hint: overampling? more epochs? different model architecture?
- how do we do early stopping and save the best model every few epochs?

# references
- Kaggle: https://www.kaggle.com/c/quora-question-pairs

# New Section
"""