# -*- coding: utf-8 -*-
"""Copy of assignment_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vmkmVkVj8D4ODStpRGOJ9AkL4_QVmhZz
"""

# This notebook is part of the workshop "modern natural language processing" run 
# by Aggregate Intellect Inc. (https://ai.science), and is released under 
# 'Creative Commons Attribution-NonCommercial-ShareAlike CC BY-NC-SA" license. 
# This material can be altered and distributed for non-commercial use with 
# reference to Aggregate Intellect Inc. as the original owner, and any material 
# generated from it must be released under similar terms. 
# (https://creativecommons.org/licenses/by-nc-sa/4.0/)

"""# download data from host"""

!wget "https://ai-modern-nlp-workshop.s3.amazonaws.com/datasets/glove.6B.zip" -O "glove.6B.50d.zip"
!unzip /content/glove.6B.50d.zip
!wget "https://ai-modern-nlp-workshop.s3.amazonaws.com/datasets/train.csv.zip" -O "train.zip"
!unzip /content/train.zip
# !wget "https://drive.google.com/uc?id=15S3OXTbRdAlYIx2_R2RnI7l2CzszQOdu" -O "train.csv"

"""we import several libraries:
- keras is used for text preprocessing
- torch is for building the LSTM neural net
- gensim is for loading pre-trained Glove word embeddings
"""

import re
import csv
import codecs
import numpy as np
import pandas as pd

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from torch.utils.data import DataLoader, TensorDataset
import torch
import torch.nn as nn
import torch.nn.functional as F

from gensim.models import KeyedVectors

"""# reading in the data
we hard code a dictionary to clean punctuated words and do the text cleaning as we read in the training file
"""

def text_to_wordlist(text):    
    """Convert words to lower case, split them, and clean punctuated words using a hard coded dictionary."""
    text = text.lower().split()
    text = " ".join(text)

    # Clean the text
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)
    
    return(text)

# read in the training data and clean the text
texts_1 = [] 
texts_2 = []
labels = []
with codecs.open('train.csv', encoding='utf-8') as f:
    reader = csv.reader(f, delimiter=',')
    header = next(reader)
    for values in reader:
        texts_1.append(text_to_wordlist(values[3]))
        texts_2.append(text_to_wordlist(values[4]))
        labels.append(int(values[5]))
print(f'Found {len(texts_1)} texts in train.csv')
print('We visualize the actual data: \n ')
print('\n', texts_1[0])
print('\n', texts_2[0])
print('\n', labels[0])

"""# tokenizing, word indexing and padding
In the following block, we do the following procedures:

1) tokenize the sentences: convert a sentence to list of words

2) word indexing: convert word(s) to integer(s)

3) padding and truncating the sentences to the same length of 30 words
"""

# tokenizing and indexing in the same step: converting sentence into words, then into integers
tokenizer = Tokenizer(num_words=200000)
tokenizer.fit_on_texts(texts_1 + texts_2) # training
sequences_1 = tokenizer.texts_to_sequences(texts_1) # predicting
sequences_2 = tokenizer.texts_to_sequences(texts_2) # predicting
print('\n')
print(f"sequences_1: {sequences_1[0]}")

word_index = tokenizer.word_index
print('\n')
print(f'Found {len(word_index)} unique tokens')
# print(word_index) # uncomment to see the word dictionary

# padding/truncating sentences to the same length of 30 words
max_sequence_length = 30
data_1 = torch.tensor(pad_sequences(sequences_1, maxlen=max_sequence_length)).cuda() # Question_1, train
data_2 = torch.tensor(pad_sequences(sequences_2, maxlen=max_sequence_length)).cuda() # Question_2, train
print('\n')
print(f"data_1: {data_1[0]}")

"""# embeddings

Because pre-trained glove embeddings is not in a format to be loaded into pytorch, we need to convert the format first
"""

!python -m gensim.scripts.glove2word2vec --input glove.6B.50d.txt --output glove.6B.50d.w2vformat.txt

"""We load the pre-trained glove embeddings into Pytorch then query the embeddings"""

model = KeyedVectors.load_word2vec_format('glove.6B.50d.w2vformat.txt')
weights = torch.FloatTensor(model.vectors)
embedding = nn.Embedding.from_pretrained(weights)
print(embedding, '\n')

# query the embeddings
# TODO: finish the below so that we can query the pre-trained embeddings. Utilize the word_index dictionary we created.
input = torch.LongTensor([233])
# print(word_index)
# other example words: 'them 136', 'company 177', 'during 233'
print(f"query the embedding: \n{embedding(input)}")

"""# creating batches of training/validation/testing data

Here we unify question 1 and question 2, length of 30 respectively, into 1 large sentence, length of 60
We will later feed this unified sentence into LSTM as input
"""

# NOTE: if we feed question 1 and question 2 separately into 2 diff network, performance would improve
# But we take a simpler approach here by unifying the 2 questions into 1 sentence, and feed it into 1 network
data = torch.cat((data_1, data_2), 1)
labels = torch.tensor(labels)
print(data_1.size(), data_2.size())
print(data.size())
print(labels.size())

"""Then, we split the data into 80-10-10 (train-val_test).

We also use the pytorch TensorDataset method to create a dataset format readable by PyTorch models,

followed by uisng DataLoader function to create batches of data.

We do these procedures for all training, validation, and testing set.
"""

# split data into trainining, validation, testing set
split_frac = 0.8
len_feat = len(data)
train_x = data[0:int(split_frac*len_feat)]
train_y = labels[0:int(split_frac*len_feat)]
temp_remain_x = data[int(split_frac*len_feat):]
temp_remain_y = labels[int(split_frac*len_feat):]
valid_x = temp_remain_x[0:int(len(temp_remain_x)*0.5)]
valid_y = temp_remain_y[0:int(len(temp_remain_y)*0.5)]
test_x = temp_remain_x[int(len(temp_remain_x)*0.5):]
test_y = temp_remain_y[int(len(temp_remain_y)*0.5):]
print(len(train_x), len(valid_x), len(test_x))
assert len(train_x) + len(valid_x) + len(test_x) == len(data), 'Two lengths not equal'

# set up torch dataset
train_data = TensorDataset(train_x, train_y)
# TODO: complete the tensor setup for validation data
valid_data =  TensorDataset(valid_x, valid_y)
test_data = TensorDataset(test_x, test_y)
print(train_data, valid_data, test_data)

# batching
batch_size = 2000
train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)
# TODO: complete the batching for validation data
valid_loader =  DataLoader(valid_data, shuffle=True, batch_size=batch_size)
test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)

# sanity check to see if we have consisntet label of 0 and 1 for our train/val/test set
print('labels:\n', pd.Series(labels.numpy()).value_counts() / len(labels), '\n')
print('train:\n', pd.Series(train_y.numpy()).value_counts() / len(train_y), '\n')
print('val:\n', pd.Series(valid_y.numpy()).value_counts() / len(valid_y), '\n')
print('test:\n', pd.Series(test_y.numpy()).value_counts() / len(test_y), '\n')

"""Here we do 1 more sanity check to see if we can get one batch of training data"""

iter_data = iter(train_loader)
temp_x, temp_y = iter_data.next()
print('One batch of input: \n', temp_x)
print('One batch of input size: ', temp_x.size())
print('\n')
print('One batch of label: \n', temp_y)
print('One batch of label size: ', temp_y.size())

"""# lstm

We build the actual LSTM.

Embedding layer takes the input. The embedding layer can be chosen as using a pre-trained embedding or trained along with the network.

Two hidden layers.

One drop out layer.

One fully connected layer.

Final sigmoid to produce the output.
"""

class LSTM(nn.Module):
    """
    The RNN model that will be used to perform Sentiment analysis.
    """

    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):
        """
        Initialize the model by setting up the layers.
        """
        super().__init__()

        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        
        # embedding and LSTM layers
        self.embedding = nn.Embedding(vocab_size, embedding_dim) # option (1): trained together with LSTM
        # self.embedding = embedding # option (2): pre-trained
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, 
                            dropout=drop_prob, batch_first=True)
        
        # dropout layer
        self.dropout = nn.Dropout(0.3)
        
        # linear and sigmoid layers
        self.fc = nn.Linear(hidden_dim, output_size)
        self.sig = nn.Sigmoid()
        

    def forward(self, x, hidden):
        """
        Perform a forward pass of our model on some input and hidden state.
        """
        batch_size = x.size(0)

        # embeddings and lstm_out
        embeds = self.embedding(x)
        
        # TODO: complete the forward propagation for LSTM
        lstm_out, hidden =  self.lstm(embeds, hidden) # NOTE: assignment section (4)
    
        # stack up lstm outputs
        # creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)
        
        # dropout and fully-connected layer
        # TODO: complete the drop out layer
        out =  self.dropout(lstm_out) # NOTE: assignment section (5)
        out = self.fc(out)
        
        # sigmoid function
        sig_out = self.sig(out)
        
        # reshape to be batch_size first
        sig_out = sig_out.view(batch_size, -1)
        # convert 2D to 1D
        sig_out = sig_out[:, -1]
        
        # return last sigmoid output and hidden state
        return sig_out, hidden
    
    
    def init_hidden(self, batch_size):
        ''' Initializes hidden state '''
        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,
        # initialized to zero, for hidden state and cell state of LSTM
        weight = next(self.parameters()).data
        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),
              weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())
        
        return hidden

"""Initialize the network here. Depending on if you use pre-trained embeddings or not, the network will be slightly different as shown below:


Option (1) trained with LSTM

LSTM(

  (embedding): Embedding(85519, 50)
  
  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, dropout=0.5)
  
  (dropout): Dropout(p=0.3)
  
  (fc): Linear(in_features=256, out_features=1, bias=True)
  
  (sig): Sigmoid()
  
)


Option (2) pre-trained

LSTM(
  (embedding): Embedding(400001, 50)
  
  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, dropout=0.5)
  
  (dropout): Dropout(p=0.3)
  
  (fc): Linear(in_features=256, out_features=1, bias=True)
  
  (sig): Sigmoid()
  
)
"""

vocab_size = len(word_index) + 1 # +1 for the 0 padding
output_size = 1
embedding_dim = 50
hidden_dim = 256
n_layers = 2
net = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)
print(net)

"""# training

Below is the training procedure.

We use binary cross entropy loss and adam optimizer.

Pytorch has the training pattern:
  - set the model to trainable
  - forward propagation
  - backpropagation
  - calculate the loss
  - run the optimizer
  
We also run a validation every epoch
"""

# training parameters
lr = 0.001
# TODO: specify binary cross entropy loss
criterion = nn.BCELoss() # NOTE: assignment section (6)
optimizer = torch.optim.Adam(net.parameters(), lr=lr)
epochs = 10
counter = 0
clip = 5
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
net.to(device)

# train for some number of epochs
for e in range(epochs):
    net.train()
    # initialize hidden state
    h = net.init_hidden(batch_size)

    # batch loop
    for inputs, labels in train_loader:
        if len(inputs) != batch_size:
          continue
        counter += 1

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        h = tuple([each.data for each in h])

        # zero accumulated gradients
        net.zero_grad()

        # get the output from the model
        inputs = inputs.type(torch.LongTensor)
        inputs = inputs.to(device)
        labels = labels.to(device)
        output, h = net(inputs, h)

        # calculate the loss and perform backprop
        # TODO: calculate the loss between predicted and labels
        loss = criterion(output.squeeze(), labels.float()) # NOTE: assignment section (7)
        loss.backward()
        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
        nn.utils.clip_grad_norm_(net.parameters(), clip)
        optimizer.step()

    # Get validation loss
    val_h = net.init_hidden(batch_size)
    val_losses = []
    val_acc = []
    net.eval()
    for inputs, labels in valid_loader:
        if len(inputs) != batch_size:
          continue

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        val_h = tuple([each.data for each in val_h])

        inputs = inputs.type(torch.LongTensor)
        inputs, labels = inputs.cuda(), labels.cuda()
        output, val_h = net(inputs, val_h)
        val_loss = criterion(output.squeeze(), labels.float())
        
        # accuracy
        output = (output > 0.5).float()
        output = output.type(torch.LongTensor)
        correct = (output.cpu() == labels.cpu()).float().sum()

        val_losses.append(val_loss.item())
        val_acc.append(correct / output.shape[0])

        net.train()
        
    print("Epoch: {}/{}...".format(e+1, epochs),
          "Step: {}...".format(counter),
          "Loss: {:.6f}...".format(loss.item()),
          "Val Loss: {:.6f}".format(np.mean(val_losses)),
          "Val Accuracy: {:.6f}".format(np.mean(val_acc)))

"""# test Set

Finally we run the model through a hold out test set.

The procedure is the same as the validation process.
"""

# check accuracy
test_h = net.init_hidden(batch_size)
test_losses = []
test_acc = []
net.eval()
for inputs, labels in test_loader:
    if len(inputs) != batch_size:
      continue

    # Creating new variables for the hidden state, otherwise
    # we'd backprop through the entire training history
    test_h = tuple([each.data for each in test_h])

    inputs = inputs.type(torch.LongTensor)
    inputs, labels = inputs.cuda(), labels.cuda()
    output, test_h = net(inputs, test_h)
    test_loss = criterion(output.squeeze(), labels.float())
    output = (output > 0.5).float()
    output = output.type(torch.LongTensor)
    correct = (output.cpu() == labels.cpu()).float().sum()

    test_losses.append(test_loss.item())
    test_acc.append(correct / output.shape[0])

net.train()
print("Test Loss: {:.6f}".format(np.mean(test_losses)),
      "\n"
      "Test Accuracy: {:.6f}".format(np.mean(test_acc))
      )

"""# challenges
- how can we improve the performance?
  - hint: overampling? more epochs? different model architecture?
- how do we do early stopping and save the best model every few epochs?

# references
- LSTM: Samarth Agrawal
- Kaggle: https://www.kaggle.com/c/quora-question-pairs
"""

